# -*- coding: utf-8 -*-
"""LSTM-autoencoder-capstone.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LbJvlSlAwvD1Yuxz_kLlwkPjhbjYBp6U

# Import Library
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import plotly.graph_objects as go
import warnings
from sklearn.exceptions import ConvergenceWarning
warnings.filterwarnings("ignore", category=UserWarning)
from tensorflow import keras
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Dropout, RepeatVector, TimeDistributed

"""##**Load Dataset**"""

data = pd.read_csv("https://raw.githubusercontent.com/daniel020901/Dataset-Capstone/master/Daily%20Household%20Transactions.csv")

data.info()

data.head()

"""###**Transform Data**"""

df = pd.DataFrame(data)

# Ubah tipe data


df['Mode'] = df['Mode'].astype('category')  # Ubah ke kategori
df['Category'] = df['Category'].astype('category')  # Ubah ke kategori
df['Subcategory'] = df['Subcategory'].astype('category')  # Ubah ke kategori
df['Income/Expense'] = df['Income/Expense'].astype('category')  # Ubah ke kategori
df['Currency'] = df['Currency'].astype('category')  # Ub

df['Date'] = df['Date'].str.replace(r'\s\d{2}:\d{2}(:\d{2})?', '', regex=True)

# Mengubah kolom 'Date' menjadi datetime
df['Date'] = pd.to_datetime(df['Date'], dayfirst=True, errors='coerce')

df.info()
print("Jumlah duplikasi: ",df.duplicated().sum())
df.describe()

data.head()

df.isnull().sum()

"""### **Cleansing data**
  Dilakukan proses pembersihan data untuk memastikan kolom Amount memiliki nilai valid dan dapat dianalisis. Langkah-langkah yang dilakukan adalah sebagai berikut:



1.   Membersihkan kolom Amount:


*   Koma (,) dan spasi ( ) dalam nilai harga dihapus menggunakan replace() dengan parameter regex=True.
*   Nilai dalam kolom harga dikonversi menjadi tipe numerik menggunakan pd.to_numeric(). Nilai yang tidak valid secara otomatis diubah menjadi NaN dengan parameter errors='coerce'.


2.   Mengisi values yang hilang pada kolom sub-category:

*   Memberikan keterangan jika nilai kosong dengan tidak diketahui






"""

data['Amount'] = data['Amount'].replace({',': '', ' ': ''}, regex=True)
data['Amount'] = pd.to_numeric(data['Amount'], errors='coerce')

# Menambahkan kategori baru 'Tidak Diketahui' ke dalam kolom 'Subcategory'
df['Subcategory'] = df['Subcategory'].cat.add_categories('Tidak Diketahui')

# Sekarang isi nilai yang hilang dengan 'Tidak Diketahui'
df['Subcategory'].fillna('Tidak Diketahui', inplace=True)

# Memeriksa hasilnya
print(df['Subcategory'].value_counts())

# Mengisi nilai yang hilang di kolom 'Subcategory' dengan 'Tidak Diketahui'
df['Subcategory'].fillna('Tidak Diketahui', inplace=True)

# Mengisi nilai yang hilang di kolom 'Note' dengan 'Tanpa Keterangan'
df['Note'].fillna('Tanpa Keterangan', inplace=True)

# Mengecek setelah pengisian nilai yang hilang
print(df.isnull().sum())

# Mengganti nilai 'Transfer-Out' menjadi 'Expense' pada kolom 'Income/Expense'
df['Income/Expense'] = df['Income/Expense'].replace('Transfer-Out', 'Expense')

# Memeriksa hasil perubahan
print(df['Income/Expense'].value_counts())

data.describe()

data.head()

"""## **Penjelasan Variabel**


1.   **Date**
* **Tipe:** datetime64[ns]
* **Deskripsi:** Menyimpan tanggal dan waktu transaksi yang tercatat. Formatnya adalah YYYY-MM-DD HH:MM:SS (misalnya, 2018-09-20 12:04:08). Kolom ini digunakan untuk menunjukkan kapan transaksi terjadi, yang penting untuk analisis berbasis waktu seperti tren bulanan atau tahunan.
2.   **Mode**
* **Tipe:** category
* **Deskripsi:** Menyimpan cara pembayaran atau metode transaksi yang digunakan, misalnya Cash, Credit Card, Debit, atau Bank Transfer. Kolom ini membantu mengkategorikan transaksi berdasarkan metode pembayaran yang digunakan.
3. **Category**
* **Tipe:** category
* **Deskripsi:** Menyimpan kategori utama pengeluaran atau pendapatan, seperti Transportation, Food, Subscription, dan sebagainya. Kolom ini digunakan untuk mengelompokkan transaksi dalam kategori umum yang memudahkan analisis pengeluaran atau pemasukan.
4. **Subcategory**
* **Tipe:** category
* **Deskripsi:** Menyimpan subkategori lebih spesifik dari kategori utama. Misalnya, dalam kategori Food, subkategori bisa berupa snacks, groceries, atau dining. Kolom ini memberikan detail lebih lanjut tentang jenis transaksi di dalam kategori tersebut.
5. **Note**
* **Tipe:** object
* **Deskripsi:** Menyimpan deskripsi atau catatan tambahan terkait transaksi. Kolom ini biasanya berisi informasi lebih rinci atau keterangan mengenai transaksi, seperti "Idli medu Vada mix 2 plates" atau "Ganesh idol". Data ini sering kali digunakan untuk memberikan konteks lebih lanjut bagi transaksi yang tercatat.
6. **Amount**
* **Tipe:** float64
* **Deskripsi:** Menyimpan jumlah uang yang terkait dengan transaksi. Nilai dalam kolom ini menunjukkan besarnya pengeluaran atau pemasukan yang tercatat, dan biasanya berupa angka desimal (misalnya, 30.0, 199.0). Data ini penting untuk perhitungan total pengeluaran atau pemasukan.
7. **Income/Expense**
* **Tipe:** category
* **Deskripsi:** Menyimpan tipe transaksi apakah itu Income (pendapatan) atau Expense (pengeluaran). Kolom ini membantu dalam mengkategorikan transaksi sebagai pemasukan atau pengeluaran yang berguna untuk analisis keuangan.
8. **Currency**
* **Tipe:** category
* **Deskripsi:** Menyimpan mata uang yang digunakan dalam transaksi, seperti INR, IDR, atau EUR. Kolom ini membantu dalam menentukan mata uang yang digunakan dalam transaksi, yang penting ketika melakukan analisis lintas negara atau konversi mata uang.

## **Visualisasi Data**



**1.   Fitur perbandingan antara Income dan Expense per bulan**
"""

df['Date'].min(), df['Date'].max()

start_date = '2015-01-01'
end_date = '2018-09-20'
df_filtered = df[(df['Date'] >= start_date) & (df['Date'] <= end_date)]

# Menambahkan kolom 'Year-Month' untuk mengelompokkan data per bulan
df_filtered['Year-Month'] = df_filtered['Date'].dt.to_period('M')

# Group by Year-Month dan Income/Expense, kemudian sum Amount
df_grouped = df_filtered.groupby(['Year-Month', 'Income/Expense'])['Amount'].sum().unstack(fill_value=0)

# Plot hasil perbandingan Income dan Expense dengan line chart
plt.figure(figsize=(14, 8))
plt.plot(df_grouped.index.astype(str), df_grouped['Expense'], label='Expense', color='red', marker='o')
plt.plot(df_grouped.index.astype(str), df_grouped['Income'], label='Income', color='blue', marker='o')

# Menambahkan judul dan label
plt.title('Perbandingan Income dan Expense per Bulan (2015-2018)', fontsize=16)
plt.xlabel('Tahun-Bulan', fontsize=12)
plt.ylabel('Jumlah (INR)', fontsize=12)

# Menambahkan legenda
plt.legend(title='Income/Expense')

# Mengatur tampilan agar tidak terpotong
plt.xticks(rotation=90)
plt.tight_layout()

# Menampilkan plot
plt.show()

"""### **Filter dataframe berdasarkan Date, Amount, Income/Expense yang hanya bernilai Expense**

1. Mengkonversi mata uang `INR` ke dalam bentuk `IDR` untuk kebutuhan aplikasi
"""

# Filter baris yang hanya memiliki nilai 'Expense' di kolom 'Income/Expense'
expense_df = df[df['Income/Expense'] == 'Expense']

# Memilih hanya kolom 'Date', 'Income/Expense', dan 'Amount'
expense_df = expense_df[['Date', 'Income/Expense', 'Amount']]

# Menampilkan hasil
print(expense_df.head())

# Tentukan nilai tukar INR ke IDR
nilai_tukar = 187.60  # 1 INR = 187.60 IDR

# Mengonversi kolom 'Amount' dari INR ke IDR dan menggantinya di kolom yang sama
expense_df['Amount'] = expense_df['Amount'] * nilai_tukar

# Menampilkan data setelah konversi
print(expense_df[['Date', 'Amount']].head())

print(expense_df.head())

"""## **Data Preprocessing**

**1.   Normalisasi Data**



"""

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler(feature_range=(0, 1))
expense_df['Amount'] = scaler.fit_transform(expense_df[['Amount']])

print(expense_df.head())

"""


**3.  Time Sequences**

*   Membuat time squences dengan `sequence_length` nilai data yang berdekatan dari `train_data`
*   `sequence_length` ditetapkan selama 30 hari



"""

from sklearn.preprocessing import MinMaxScaler
import numpy as np
import pandas as pd

# Menyortir data berdasarkan tanggal
expense_df = expense_df.sort_values(by='Date')

# Menggunakan hanya kolom 'Amount' untuk LSTM
amount_data = expense_df['Amount'].values

# Fungsi untuk membuat sequence data
def create_sequence(data, sequence_length):
    sequences = []
    for i in range(len(data) - sequence_length):
        sequences.append(data[i:i + sequence_length])
    return np.array(sequences)

# Menentukan panjang sequence
sequence_length = 30  # 30 hari berturut-turut untuk setiap input sequence
sequences = create_sequence(amount_data, sequence_length)

# Membagi data menjadi training dan testing (80% train, 20% test)
train_size = int(len(sequences) * 0.8)
train_data, test_data = sequences[:train_size], sequences[train_size:]

# Membagi menjadi input (X) dan output (y)
X_train, y_train = train_data[:, :-1], train_data[:, -1]
X_test, y_test = test_data[:, :-1], test_data[:, -1]

# Menyesuaikan dimensi input untuk LSTM: (samples, timesteps, features)
X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

# Output untuk verifikasi
print(f'Train data shape: {X_train.shape}, {y_train.shape}')
print(f'Test data shape: {X_test.shape}, {y_test.shape}')

"""**2. Train- test split**

*   Membagi data menjadi train (80%) dan test (20%) untuk evaluasi model nanti.



"""

train_data[0]

"""Nilai-nilai seperti ini (misalnya `3.20002560e-05`, `1.59201274e-03`, `7.20005760e-05`) adalah hasil dari normalisasi menggunakan `MinMaxScaler`, yang mengubah nilai pengeluaran menjadi skala antara 0 dan 1. Nilai ini tergolong sangat kecil, dan ini adalah hal yang diharapkan dari `MinMaxScaler` jika data asli memiliki rentang nilai yang lebih besar."""

np.random.seed(24)
tf.random.set_seed(24)

"""## **Build a model**


*   Kami akan membangun model Autoencoder LSTM. Model akan mengambil masukan berbentuk (`batch_size`, `sequence_length`, `num_features`) dan mengembalikan keluaran dengan bentuk yang sama. Dalam hal ini, `sequence_length` adalah 30 dan `num_features` adalah 1.



"""

print(X_train.shape)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dropout, RepeatVector, TimeDistributed, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2

model = Sequential()

# Encoder dengan regularisasi L2
model.add(LSTM(50, activation='sigmoid', input_shape=(X_train.shape[1], X_train.shape[2]),
               return_sequences=True, kernel_regularizer=l2(0.001)))
model.add(Dropout(rate=0.2))

model.add(LSTM(24, activation='sigmoid', return_sequences=False, kernel_regularizer=l2(0.001)))
model.add(Dropout(rate=0.3))

# Repeat Vector
model.add(RepeatVector(X_train.shape[1]))

# Decoder
model.add(LSTM(12, activation='sigmoid', return_sequences=True, kernel_regularizer=l2(0.001)))
model.add(Dropout(rate=0.3))

model.add(TimeDistributed(Dense(X_train.shape[2])))

# Compile model
model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')
model.summary()

from tensorflow.keras.callbacks import EarlyStopping

# Melatih model
history = model.fit(
    X_train,  # Data training (sequences)
    X_train,  # Target sama dengan input karena Autoencoder
    epochs=20,  # Mengurangi jumlah epochs untuk mempercepat eksperimen
    batch_size=32,  # Batch size tetap sesuai
    validation_split=0.2,  # Menggunakan 20% dari data training untuk validasi
    callbacks=[
        EarlyStopping(monitor='val_loss', patience=5, mode='min')  # Berhenti jika validasi loss tidak membaik
    ],
    shuffle=False  # Memastikan urutan data dipertahankan untuk data time-series
)

plt.plot(history.history['loss'], label='Training loss')
plt.plot(history.history['val_loss'], label='Validation loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend();

# Mean Absolute Error loss
X_train_pred = model.predict(X_train)
train_mae_loss = np.mean(np.abs(X_train_pred - X_train), axis=1)

plt.hist(train_mae_loss, bins=50)
plt.xlabel('Train MAE loss')
plt.ylabel('Number of Samples');

# Set reconstruction error threshold
threshold = np.percentile(train_mae_loss, 99)

print('Reconstruction error threshold:',threshold)

"""## **Predict anomaly berdasarkan test_data menggunakan treshold**"""

X_test_pred = model.predict(X_test, verbose=1)
test_mae_loss = np.mean(np.abs(X_test_pred-X_test), axis=1)

plt.hist(test_mae_loss, bins=50)
plt.xlabel('Test MAE loss')
plt.ylabel('Number of samples')

# Menentukan anomali dengan threshold berdasarkan persentil ke-90
anomaly_percentile = train_mae_loss > threshold


# Menampilkan jumlah anomali yang terdeteksi
print(f"Jumlah anomali berdasarkan persentil 99: {np.sum(anomaly_percentile)}")

import matplotlib.pyplot as plt

# Plot hasil deteksi anomali berdasarkan persentil 99
plt.figure(figsize=(10, 6))
plt.scatter(range(len(train_mae_loss)), train_mae_loss, c=anomaly_percentile, cmap='coolwarm', label='Normal Data')
plt.xlabel('Index')
plt.ylabel('Reconstruction Error (Loss)')
plt.title('Anomaly Detection Based on 99th Percentile')
plt.legend()
plt.show()

anomaly_df = pd.DataFrame(test_data[sequence_length :])  # Hanya mengambil data setelah TIME_STEPS
anomaly_df['loss'] = test_mae_loss[sequence_length :]  # Mengambil nilai loss setelah TIME_STEPS
anomaly_df['threshold'] = threshold  # Gunakan nilai threshold yang sama untuk seluruh baris
anomaly_df['anomaly'] = anomaly_df['loss'] > anomaly_df['threshold']

test_mae_loss = test_mae_loss.flatten()

# Membuat histogram dari test_mae_loss
plt.hist(test_mae_loss, bins=50, color='skyblue', edgecolor='black')
plt.xlabel('Test MAE Loss')
plt.ylabel('Number of Samples')

# Menambahkan garis vertikal untuk threshold (misalnya, berdasarkan kuartil atau nilai tertentu)
threshold = np.percentile(test_mae_loss, 95)  # 95th percentile sebagai threshold
plt.axvline(threshold, color='red', linestyle='dashed', linewidth=2)

# Menambahkan teks di threshold
plt.text(threshold + 0.005, 20, f'Threshold: {threshold:.3f}', color='red')

plt.show()

# Menurunkan threshold
threshold = 0.014

# Membuat array threshold dengan panjang yang sesuai dengan data uji
threshold_array = np.full(len(test_mae_loss), threshold)

# Threshold array dengan panjang sesuai data uji
threshold_array = np.full(len(test_mae_loss), threshold)

# DataFrame untuk deteksi anomali
anomaly_df = pd.DataFrame({
    'Date': expense_df['Date'][sequence_length:sequence_length + len(test_mae_loss)].values,  # Kolom Date
    'Amount': expense_df['Amount'][sequence_length:sequence_length + len(test_mae_loss)].values,  # Kolom Amount
    'loss': test_mae_loss,  # Nilai MAE Loss
    'threshold': threshold_array,  # Threshold
    'anomaly': test_mae_loss > threshold_array  # True jika anomali
})

# Lihat hasil awal
print(anomaly_df.head())

anomalies = anomaly_df[anomaly_df['anomaly'] == True]
print("Anomalies detected:")
print(anomalies)

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.plot(anomaly_df['Date'], anomaly_df['loss'], label='Loss')
plt.plot(anomaly_df['Date'], anomaly_df['threshold'], label='Threshold', color='red')
plt.xticks(rotation=45)
plt.legend()
plt.title('Anomaly Detection: Loss vs Threshold')
plt.xlabel('Date')
plt.ylabel('Loss')
plt.show()

fig = go.Figure()
fig.add_trace(go.Scatter(x=anomaly_df['Date'], y=anomaly_df['loss'], name='Test loss'))
fig.add_trace(go.Scatter(x=anomaly_df['Date'], y=anomaly_df['threshold'], name='Threshold'))
fig.update_layout(showlegend=True, title='Test loss vs. Threshold')
fig.show()

test_labels = anomaly_df['anomaly'].map({True: 1, False: 0}).values

# Menampilkan 10 label pertama
print(test_labels[:10])  # Menampilkan 10 label pertama

print(f"Jumlah anomali berdasarkan prediksi: {np.sum(test_mae_loss > threshold)}")

# Label prediksi (1 = anomali, 0 = normal)
y_pred = (test_mae_loss > threshold).astype(int)

# Contoh label asli (1 = anomali, 0 = normal)
y_true = test_labels  # Label asli dari dataset uji

from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix

# Precision
precision = precision_score(y_true, y_pred)

# Recall
recall = recall_score(y_true, y_pred)

# F1-Score
f1 = f1_score(y_true, y_pred)

# Confusion Matrix
cm = confusion_matrix(y_true, y_pred)

print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-Score: {f1:.4f}")
print(f"Confusion Matrix:\n{cm}")

anomalies = anomaly_df.loc[anomaly_df['anomaly'] == True]
anomalies.head()

plt.figure(figsize=(15, 8))
plt.plot(anomaly_df['Date'], anomaly_df['Amount'], label='Normal Data', color='blue')

# Menambahkan anomaly
plt.scatter(anomalies['Date'], anomalies['Amount'], color='red', label='Anomalies', s=50)

# Label dan keterangan
plt.title('Anomaly Detection in Expense Data')
plt.xlabel('Date')
plt.ylabel('Amount')
plt.legend()
plt.grid(True)

# Tampilkan plot
plt.show()

# model.save('model_v1.h5')  # Tambahkan versi dalam nama file

print(model.input_shape)

import numpy as np
from sklearn.preprocessing import MinMaxScaler

def test_manual_data(input_date, input_amount, scaler, model, sequence_length, threshold):
    """
    Menguji data manual untuk mendeteksi anomali.
    """
    try:
        # Normalisasi jumlah
        normalized_amount = scaler.transform([[input_amount]])  # Normalisasi input_amount

        # Simulasi sequence dengan data input
        sequence = np.zeros((sequence_length, 1))  # Membuat sequence dengan panjang sesuai (sequence_length)
        sequence[-1] = normalized_amount  # Menempatkan normalized_amount pada posisi terakhir sequence

        # Reshape untuk sesuai dengan input model (1, sequence_length, 1)
        sequence = sequence.reshape((1, sequence_length, 1))

        # Prediksi dengan model
        predicted_sequence = model.predict(sequence, verbose=0)

        # Hitung loss
        loss = np.mean(np.abs(predicted_sequence - sequence))

        # Deteksi anomali
        is_anomaly = loss > threshold

        # Mengembalikan hasil
        return {
            "Input Date": input_date,
            "Input Amount": input_amount,
            "Normalized Amount": normalized_amount[0][0],
            "Reconstruction Loss": loss,
            "Threshold": threshold,
            "Anomaly": is_anomaly
        }

    except Exception as e:
        return {"Error": str(e)}

# Contoh Pengujian
input_date = "2023-08-21"  # Tanggal input manual
input_amount = 1000000  # Jumlah dalam IDR

# Pastikan scaler, model, sequence_length, dan threshold sudah diinisialisasi sebelumnya
try:
    result = test_manual_data(input_date, input_amount, scaler, model, 29, 0.05)

    # Menampilkan hasil
    print("Hasil Pengujian Manual:")
    for key, value in result.items():
        print(f"{key}: {value}")
except NameError as e:
    print(f"Error: Pastikan scaler, model, sequence_length, dan threshold sudah diinisialisasi dengan benar.")